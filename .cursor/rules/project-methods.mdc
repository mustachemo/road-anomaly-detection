---
description:
globs:
alwaysApply: true
---
## 0 . Context at a Glance

| Item | Detail |
|------|--------|
| **Dataset** | **R**adio‑observatory **O**utlier/Anomaly **D**ataset (ROAD) – 7 050 autocorrelation spectrograms, each resized to **256 × 256** (time × frequency) bins and stored as magnitude‑only HDF5 tensors :contentReference[oaicite:0]{index=0}. |
| **Classes** | 1 “Normal” + 9 anomaly classes (data‑loss, high‑noise element, oscillating tile, galactic plane, lightning, etc.) with heavy imbalance – e.g., *Normal* = 4 687 vs. *Oscillating tile* = 56 :contentReference[oaicite:1]{index=1}. |
| **Problem framing** | Binary *,Is‑this‑spectrogram‑abnormal?* detector that must tolerate unseen anomalies and minimise **false negatives** (F₂ score target ≈ 0.9+, FPR ≤ 2 %) :contentReference[oaicite:2]{index=2}. |

---

## 1 . Core Idea

Learn the support of *normal* telescope behaviour with a **One‑Class SVM (OC‑SVM)**; any sample outside that support is flagged as anomalous.

Why OC‑SVM for ROAD?

* **High‑dimensional features** (65 536 px) – kernel SVM handles this well.
* **Few anomaly labels** – OC‑SVM needs only normal data; rare classes become *out‑of‑support* by definition.
* **Interpretable control** over outlier rate via **ν** (fraction of support vectors/outliers).

---

## 2 . Data Pipeline (Road‑Specific)

```mermaid
flowchart LR
    A[HDF5 file] --> B[Clip 1‑99 %ile & log‑magnitude]:::step
    B --> C[Min‑Max 0‑1 normalisation]:::step
    C --> D[Flatten 256×256 → 65 536‑D]:::step
    D --> E[StandardScaler (μ=0, σ=1)]:::step
    E --> F[PCA (95 % var, ≈ 100‑150 dims)]:::step
    F --> G[One‑Class SVM]:::model
classDef step fill:#f5f5f5,stroke:#999,font-size:12px;
classDef model fill:#cfe3ff,stroke:#036,font-weight:bold;
````

| Step               | Justification                                                                                                           |
| ------------------ | ----------------------------------------------------------------------------------------------------------------------- |
| **Clipping & log** | Prevents gradient explosions and preserves weak features, exactly as the ROAD authors did .                             |
| **Normalise 0‑1**  | Removes scale bias between LBA/HBA bands.                                                                               |
| **PCA**            | 65 k‑dim vectors → tractable (≈ O(N²) kernel matrix) while retaining morphology; keeps fits < 8 GB RAM for 5 k normals. |

---

## 3 . Model Configuration

| Parameter                   | Rationale                                                                                                                  | Grid / Default                                      |
| --------------------------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **Kernel**                  | **RBF**: flexible local decision boundary – necessary for the diverse morphologies (lightning vs. galactic plane)          | `"rbf"`                                             |
| **γ (gamma)**               | Controls locality.  Use sklearn’s `gamma="scale"` as centre, then log‑scale search                                         | {0.1, 0.3, 1, 3, 10} × `1 / (n_features × X.var())` |
| **ν (nu)**                  | Upper bound on training outliers ≈ expected false‑neg. rate.  For ROAD we target **1 %** (FPR 2 % budget incl. test drift) | {0.005, 0.01, 0.02, 0.05}                           |
| **Tolerance / cache\_size** | `tol=1e‑4`, `cache_size=4 096` MB (speed)                                                                                  |                                                     |

---

## 4 . Training & Validation Strategy

* **Train set**: all *Normal* samples (≈ 4 687).
* **Validation split**: 10 % stratified on station + frequency band to capture both LBA and HBA normals.
* **Early stopping**: pick (γ, ν) giving best **AUPRC** on a *shadow* validation of *Normal vs. {1 % anomalies}*.
* **Test protocol**:

  * 10 bootstrap replicas matching original class frequencies (ROAD authors’ practice) .
  * Report **F₂**, **AUROC**, **FPR\@T\*** where T\* is threshold that maximises F₂ on validation curve, then frozen.

> **Thresholding** – use signed distance output; threshold chosen from validation PR curve that maximises F₂ (β = 2) – same metric ROAD uses for telescope ops .

---

## 5 . Coding Skeleton (inside `src/`)

```python
# src/models/one_class_svm.py
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import OneClassSVM
import joblib, yaml

def build_ocsvm(cfg_path: str = "config/train.yaml") -> Pipeline:
    cfg = yaml.safe_load(open(cfg_path))
    return Pipeline([
        ("scale", StandardScaler()),
        ("pca",   PCA(n_components=cfg["pca_var"])),           # 0.95
        ("svm",   OneClassSVM(kernel="rbf",
                              gamma=cfg["gamma"],
                              nu=cfg["nu"],
                              cache_size=4096,
                              tol=1e-4))
    ])

def train(X_train, cfg_path):
    pipe = build_ocsvm(cfg_path)
    pipe.fit(X_train)
    joblib.dump(pipe, "models/ocsvm_road.joblib")
```

`config/train.yaml` example:

```yaml
pca_var: 0.95
gamma: "scale"
nu: 0.01
```

---

## 6 . Evaluation Script Snippet

```python
from sklearn.metrics import precision_recall_curve, fbeta_score
import numpy as np, joblib, yaml, h5py

def load_road_test(h5_path="road.h5"):
    with h5py.File(h5_path) as f:
        X = f["data"][:]           # (N, 256, 256)
        y = f["label"][:]          # 0=Normal, 1–9 anomalies
    return preprocess(X), (y != 0).astype(int)

def find_opt_threshold(scores, y_val, beta=2):
    prec, rec, thr = precision_recall_curve(y_val, scores)
    f2 = (1+beta**2) * (prec*rec) / (beta**2*prec + rec + 1e-12)
    return thr[np.argmax(f2)]

model = joblib.load("models/ocsvm_road.joblib")
X_test, y_test = load_road_test()
scores = -model.decision_function(X_test)        # higher = more anomalous
T_star = find_opt_threshold(scores_val, y_val)   # from val split
y_pred = (scores > T_star).astype(int)
print("F2 =", fbeta_score(y_test, y_pred, beta=2))
```

---

## 7 . Why This Should Work (Limitations & Mitigations)

| Challenge                                              | Mitigation                                                                             |
| ------------------------------------------------------ | -------------------------------------------------------------------------------------- |
| **Class imbalance** – Some anomalies < 100 samples     | Treated as *OOС*, hence no training leakage.                                           |
| **LBA vs. HBA distribution shift**                     | Clip + log scale and PCA learnt on both bands; validation split stratified.            |
| **High dimensionality**                                | PCA to 95 % variance keeps kernel matrix tractable.                                    |
| **Temporal correlation within observations**           | Bootstrap resampling by *observation id* to avoid leakage across time‑adjacent frames. |
| **Non‑stationary telescope behaviour (new RFI epoch)** | Periodic re‑train with sliding 1‑month window, ν warm‑started from last fit.           |

---

## 8 . Next Experiments (roadmap)

1. **Binary SVM** with class‑weighted `SVC` (Normal vs. *known* anomalies) – sanity check.
2. **Hybrid** – OC‑SVM gate + multiclass `SVC` softmax to classify anomaly type (similar to ROAD’s SSL+CLS fusion).
3. **Nyström Approximation** – speed up to real‑time (< 1 ms) by approximating RBF kernel with 512 landmarks.
4. **Station‑aware OC‑SVMs** – train separate models per station subset; ensemble voting reduces false negatives for geographically local events like lightning .

---

### TL;DR

1. **Pre‑process** ROAD spectrograms exactly as the paper (clip→log→resize 256²).
2. **Pipe**: `StandardScaler → PCA(95 %) → OneClassSVM(RBF)`.
3. **Tune** γ & ν on a stratified validation set using F₂‑optimised threshold from PR curve.
4. **Report** F₂ & FPR on 10 bootstrap splits matching ROAD class frequencies.
5. **Iterate** with Nyström or per‑station ensembles if latency or recall targets are not met.

This blueprint keeps us faithful to ROAD’s quirks while exploiting the strengths of SVMs for high‑dimensional, label‑sparse anomaly detection.

```
```
